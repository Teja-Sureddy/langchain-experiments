"""
It performs an action or run a function. LLM agent decides when and how to use it.
Their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.

Search: DuckDuckGoSearchResults, BraveSearch, TavilySearchResults, GoogleSearchResults
Code Interpreter: PythonREPLTool, BearlyInterpreterTool
Productivity: GmailSendMessage, GmailGetThread, SlackSendMessage
Web Browsing: RequestsGetTool, RequestsPostTool, ExtractTextTool, ClickTool (browser)
Database: QuerySQLDatabaseTool, VectorStoreQATool, MongoDBStore
Others: ShellTool, YouTubeSearchTool, WikipediaQueryRun
"""
import os
import textwrap
from dotenv import load_dotenv
import warnings
from langchain_core._api.deprecation import LangChainDeprecationWarning

# -------------------- Search --------------------
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain


def search(model: str = 'gemma3'):
    llm = ChatOllama(model=model)
    prompt_template = PromptTemplate(
        input_variables=["search_results", "question"],
        template="""
            You are a helpful assistant. Based on the search results below, provide a clear, 
            organized answer to the user's question.

            Question: {question}

            Search Results: {search_results}

            Please provide:
            1. Summarize what is it
            2. Give me 2 advantages
            3. Give me 2 disadvantages
        """
    )
    chain = LLMChain(llm=llm, prompt=prompt_template)

    while True:
        prompt = input('\nEnter prompt: ') or "Give me everything about numpy"
        search_results = DuckDuckGoSearchResults().invoke(prompt)
        response = chain.run(question=prompt, search_results=search_results)
        print(textwrap.fill(response, width=100))


# -------------------- Code Interpreter --------------------
from langchain_experimental.tools import PythonREPLTool
from langchain.agents import initialize_agent, AgentType


def code_interpreter(model: str = 'gemma3'):
    llm = ChatOllama(model=model)
    python_repl = PythonREPLTool()
    agent = initialize_agent([python_repl], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, max_iterations=5)

    while True:
        prompt = input('\nEnter prompt: ') or "Plot y=x^2 from 0 to 10"
        response = agent.invoke({"input": prompt})
        print(textwrap.fill(response['output'], width=100))


# -------------------- Web Browsing --------------------
from langchain_community.tools import RequestsGetTool
from langchain_community.utilities import RequestsWrapper


def web_browsing(model: str = 'gemma3'):
    llm = ChatOllama(model=model)
    request_tool = RequestsGetTool(requests_wrapper=RequestsWrapper(), allow_dangerous_requests=True)
    agent = initialize_agent([request_tool], llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, max_iterations=10)

    while True:
        prompt = input('\nEnter prompt: ') or ("Use 'https://dummyjson.com/users/1' to get the json formatted data and "
                                               "provide me the email of the user.")
        response = agent.invoke({"input": prompt})
        print(textwrap.fill(response['output'], width=100))


# -------------------- Database --------------------
from sqlalchemy import create_engine
from langchain_community.tools import QuerySQLDataBaseTool
from langchain_community.utilities import SQLDatabase


def database(model: str = 'gemma3'):
    user, password = os.environ.get('POSTGRES_USERNAME'), os.environ.get('POSTGRES_PASSWORD')
    engine = create_engine(f"postgresql+psycopg2://{user}:{password}@localhost:5432/langchain")
    db = SQLDatabase(engine)
    sql_tool = QuerySQLDataBaseTool(db=db)

    llm = ChatOllama(model=model)
    agent = initialize_agent(tools=[sql_tool], llm=llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, max_iterations=5)

    while True:
        prompt = input('\nEnter prompt: ') or "Get the users older than 40, and give me only their names"
        response = agent.invoke({"input": prompt})
        print(response['output'])


# -------------------- Custom --------------------
from langchain_core.tools import tool
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder


@tool
def validate_user(user_id: int) -> bool:
    """Validate a user by checking if their user ID exists."""
    return user_id in (1, 2, 3)


def custom(model: str = 'llama3.2'):
    """
    Lets the LLM call Python functions to get real-time answers.
    """
    llm = ChatOllama(model=model)
    tools = [validate_user]
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "Make the response short."),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad")  # tool calls
    ])
    agent = create_tool_calling_agent(llm, tools, prompt_template)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)

    while True:
        prompt = input("\nEnter prompt: ") or "Can you validate user 1?"
        response = agent_executor.invoke({"input": prompt})
        print(response["output"])


load_dotenv()
warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)

if __name__ == "__main__":
    # search()
    # code_interpreter()
    # web_browsing()
    # database()
    # custom()
    pass
